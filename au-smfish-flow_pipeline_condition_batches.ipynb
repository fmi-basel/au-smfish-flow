{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of Packages\n",
    "Here you load all necessary packages to run your code.\n",
    "As you can see it imports functions from the Segmentation_function.py file.\n",
    "\n",
    "I would recomend to paste these package imports as well as the ones from the Segmentation_function.py into chatgpt and ask which packages to install via pip install for your conda environment to get the code running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "from tifffile import imread, imwrite\n",
    "from cellpose import models\n",
    "import matplotlib.pyplot as plt\n",
    "from bigfish.detection import get_object_radius_pixel\n",
    "from skimage.measure import label, regionprops\n",
    "from Segmentation_function import (\n",
    "    load_nd_metadata, extract_tiff_metadata, generate_mips,\n",
    "    run_cellpose_segmentation, extract_nuclei_properties, build_anndata,\n",
    "    segment_cytoplasm_by_cellpose, detect_smfish_spots,\n",
    "    generate_segmentation_report, process_sample, analyze_sample\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=anndata._core.aligned_df.ImplicitModificationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Functions \n",
    "\n",
    "FYI here I define some function to run over several folders instead of the single pieces. \n",
    "I guess in the future this will be hidden in the segmentaiton_function.py file and only the run command below will be visible.\n",
    "But for now this can be usefull to tweack the parameters like the SNR ratio of 3.0, the cropped area or other things like channel name etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_samples(folder_path: str, crop_coords=(800, 1200, 800, 1200), snr_threshold: float = 3.0):\n",
    "    \"\"\"\n",
    "    Process all samples in a folder that contain .nd and .stk files.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Root directory containing sample files.\n",
    "        crop_coords (tuple): Coordinates for cropping spot region in segmentation report.\n",
    "        snr_threshold (float): SNR threshold for filtering smFISH spots.\n",
    "    \"\"\"\n",
    "    files = os.listdir(folder_path)\n",
    "    sample_names = set()\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".nd\"):\n",
    "            base = file.replace(\".nd\", \"\")\n",
    "            stk1 = f\"{base}_w1conf561Virtex.stk\"\n",
    "            stk2 = f\"{base}_w2conf405Virtex.stk\"\n",
    "            if stk1 in files and stk2 in files:\n",
    "                sample_names.add(base)\n",
    "\n",
    "    if not sample_names:\n",
    "        print(\"‚ö†Ô∏è No complete samples (.nd + .stk files) found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîé Found {len(sample_names)} samples to process: {sorted(sample_names)}\")\n",
    "\n",
    "    for sample_name in sorted(sample_names):\n",
    "        try:\n",
    "            process_sample(folder_path, sample_name, crop_coords=crop_coords, snr_threshold=snr_threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {sample_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_timepoints(root_folder: str, crop_coords=(800, 1200, 800, 1200), snr_threshold: float = 3.0):\n",
    "    \"\"\"\n",
    "    Process all timepoint subfolders inside a root directory.\n",
    "\n",
    "    Parameters:\n",
    "        root_folder (str): Path to root folder (e.g., /path/to/Eglantine).\n",
    "        crop_coords (tuple): Crop coordinates for segmentation report.\n",
    "        snr_threshold (float): SNR threshold for smFISH filtering.\n",
    "    \"\"\"\n",
    "    # List valid subfolders\n",
    "    subfolders = sorted([\n",
    "        f for f in os.listdir(root_folder)\n",
    "        if os.path.isdir(os.path.join(root_folder, f)) and not f.startswith(\"._\")\n",
    "    ])\n",
    "\n",
    "    if not subfolders:\n",
    "        print(\"‚ö†Ô∏è No valid timepoint subfolders found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üïí Found {len(subfolders)} timepoints: {subfolders}\\n\")\n",
    "\n",
    "    # Loop through each timepoint folder\n",
    "    for tp in subfolders:\n",
    "        tp_path = os.path.join(root_folder, tp)\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üß™ Processing timepoint folder: {tp}\")\n",
    "        print(\"=\" * 70)\n",
    "        try:\n",
    "            process_all_samples(tp_path, crop_coords=crop_coords, snr_threshold=snr_threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process timepoint '{tp}': {e}\")\n",
    "        print()  # Line break for clarity between timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Wrapper: run all timepoints in all replicates of one condition =====\n",
    "\n",
    "def _list_clean_subfolders(parent, exclude={\"analysis_output\", \"Analysis\", \"results\"}):\n",
    "    return sorted([\n",
    "        d for d in os.listdir(parent)\n",
    "        if os.path.isdir(os.path.join(parent, d))\n",
    "        and not d.startswith((\"._\", \".\"))\n",
    "        and d not in exclude\n",
    "    ])\n",
    "\n",
    "def process_condition(condition_folder: str, crop_coords=(800, 1200, 800, 1200), snr_threshold: float = 3.0):\n",
    "    \"\"\"\n",
    "    Run the full segmentation pipeline for every replicate and timepoint inside a condition.\n",
    "\n",
    "    Directory layout expected:\n",
    "        condition_folder/\n",
    "          replicate 1/\n",
    "            0h/, 1h/, ..., Nh/\n",
    "          replicate 2/\n",
    "            0h/, ..., Nh/\n",
    "          ...\n",
    "\n",
    "    This calls your existing `process_all_timepoints()` for each replicate.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(condition_folder):\n",
    "        print(f\"‚ùå Condition path not found: {condition_folder}\")\n",
    "        return\n",
    "\n",
    "    replicates = _list_clean_subfolders(condition_folder)\n",
    "    if not replicates:\n",
    "        print(f\"‚ö†Ô∏è No replicate folders found in {condition_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üß™ Condition: {os.path.basename(condition_folder)} ‚Äî Found {len(replicates)} replicates: {replicates}\\n\")\n",
    "    for rep in replicates:\n",
    "        rep_path = os.path.join(condition_folder, rep)\n",
    "        print(\"#\" * 80) \n",
    "        print(f\"üîÅ Processing replicate: {rep}\")\n",
    "        print(\"#\" * 80)\n",
    "        try:\n",
    "            process_all_timepoints(rep_path, crop_coords=crop_coords, snr_threshold=snr_threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed replicate '{rep}': {e}\")\n",
    "        print()  # spacing\n",
    "\n",
    "\n",
    "def analyze_condition(condition_folder: str, snr_threshold: float = 3.0):\n",
    "    \"\"\"\n",
    "    Run the analysis stage for every replicate (and all timepoints within) of a condition.\n",
    "\n",
    "    This calls your existing `analyze_all_timepoints()` for each replicate.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(condition_folder):\n",
    "        print(f\"‚ùå Condition path not found: {condition_folder}\")\n",
    "        return\n",
    "\n",
    "    replicates = _list_clean_subfolders(condition_folder)\n",
    "    if not replicates:\n",
    "        print(f\"‚ö†Ô∏è No replicate folders found in {condition_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìä Analyzing condition: {os.path.basename(condition_folder)} ‚Äî {len(replicates)} replicates: {replicates}\\n\")\n",
    "    for rep in replicates:\n",
    "        rep_path = os.path.join(condition_folder, rep)\n",
    "        print(\"#\" * 80)\n",
    "        print(f\"üì¶ Analyzing replicate: {rep}\")\n",
    "        print(\"#\" * 80)\n",
    "        try:\n",
    "            analyze_all_timepoints(rep_path, snr_threshold=snr_threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed analysis for replicate '{rep}': {e}\")\n",
    "        print()  # spacing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Run Function \n",
    "This function will run the segmentation and spot detection in a folder that has subfolders as timepoitns (1h,2h .... 8h) and will process every timepoint + every sample within.\n",
    "So you have to add your new folder path to the root_folder variable (please always copy your data and dont perform on raw data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#This is my Code for a single folder (1 timepoitn like the 2h only)\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"/Volumes/Project_PhD/6_Coding/Eglante/2h\"\n",
    "    process_all_samples(folder)\n",
    "\n",
    "\n",
    "# This is the code for multiple timepoints here you give the path to your folder wiht all the timepoint folders inside the smapels everthing else goes automatic\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder =  root_folder# Folder containing 0h, 1h, ..., 10h\n",
    "    process_all_timepoints(root_folder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Run Function definition\n",
    "\n",
    "This code defines the  analysis on all segmented images e.g. is the dot in or outside the nuclei etc. Later I should hide this into the main Run code but for now its like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_samples(folder_path: str, snr_threshold: float = 3.0):\n",
    "    \"\"\"\n",
    "    Analyze all .h5ad samples in the given folder.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path containing .h5ad files.\n",
    "        snr_threshold (float): Minimum SNR threshold for filtering spots.\n",
    "    \"\"\"\n",
    "    h5ad_files = [\n",
    "        os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "        if f.endswith(\"_nuclei_spots.h5ad\") and not f.startswith(\"._\")\n",
    "    ]\n",
    "\n",
    "    if not h5ad_files:\n",
    "        print(\"‚ö†Ô∏è No .h5ad files found for analysis.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîç Found {len(h5ad_files)} samples for analysis.\")\n",
    "\n",
    "    for path in sorted(h5ad_files):\n",
    "        sample_name = os.path.basename(path).replace(\"_nuclei_spots.h5ad\", \"\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üì¶ Analyzing Sample: {sample_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        try:\n",
    "            analyze_sample(path, snr_threshold=snr_threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to analyze {sample_name}: {e}\")\n",
    "        print()  # Empty line between samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_timepoints(root_folder: str, snr_threshold: float = 3.0):\n",
    "    \"\"\"\n",
    "    Analyze all timepoint subfolders inside a root directory, each containing .h5ad files.\n",
    "\n",
    "    Parameters:\n",
    "        root_folder (str): Path to root folder (e.g., /path/to/Eglantine).\n",
    "        snr_threshold (float): Minimum signal-to-background ratio for filtering spots.\n",
    "    \"\"\"\n",
    "    # Only include folders that look like timepoints; exclude analysis folders and dotfiles\n",
    "    subfolders = sorted([\n",
    "        f for f in os.listdir(root_folder)\n",
    "        if os.path.isdir(os.path.join(root_folder, f))\n",
    "        and not f.startswith((\"._\", \".\"))\n",
    "        and f not in {\"analysis_output\", \"Analysis\", \"results\"}\n",
    "    ])\n",
    "\n",
    "    if not subfolders:\n",
    "        print(\"‚ö†Ô∏è No valid timepoint subfolders found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üïí Found {len(subfolders)} timepoints to analyze: {subfolders}\\n\")\n",
    "\n",
    "    for tp in subfolders:\n",
    "        tp_output_path = os.path.join(root_folder, tp, \"analysis_output\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üß™ Analyzing timepoint: {tp}\")\n",
    "        print(\"=\" * 70)\n",
    "        if not os.path.isdir(tp_output_path):\n",
    "            print(f\"‚ö†Ô∏è Skipping '{tp}': no analysis_output at {tp_output_path}\\n\")\n",
    "            continue\n",
    "        try:\n",
    "            analyze_all_samples(tp_output_path, snr_threshold=snr_threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to analyze timepoint '{tp}': {e}\")\n",
    "        print()  # Newline for spacing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Run code\n",
    "\n",
    "Again here replace the folder path to the one before where you run the previous code. It will than analyse each .h5ad file in the timepoints and sample folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'path/to/your/data/folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This is my Code for a single folder (1 timepoitn like the 2h only)\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = \"/Volumes/Project_PhD/6_Coding/Eglante/2h\"  # contains 0h, 1h, ..., 10h\n",
    "    analyze_all_samples(root_folder, snr_threshold=3)\n",
    "\n",
    "\n",
    "# This is the code for multiple timepoints here you give the path to your folder wiht all the timepoint folders inside the smapels everthing else goes automatic\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = root_folder\n",
    "    analyze_all_timepoints(root_folder, snr_threshold=3)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "# ===== Example usage =====\n",
    "if __name__ == \"__main__\":\n",
    "    # Set one condition folder at a time and run:\n",
    "    condition_folder = root_folder   # e.g., \".../Suntag/condition1\"\n",
    "    process_condition(condition_folder, crop_coords=(800, 1200, 800, 1200), snr_threshold=3.0)\n",
    "\n",
    "    # When segmentation finished (h5ad + outputs exist), run the analysis:\n",
    "    analyze_condition(condition_folder, snr_threshold=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging of Analysis data\n",
    "\n",
    "Due to the way I coded it, all samples have seperate reports with seperate .csv and .h5ad files. This code runs over everything and combines all .csv into one experiment .csv => makes it easy to handle and plot later.\n",
    "\n",
    "Maybe change here the output_name in the paramters otherwise its the experiment_merged name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import anndata\n",
    "import pandas as pd\n",
    "\n",
    "def merge_experiment(root_folder: str, output_name: str = \"experiment_merged\"):\n",
    "    \"\"\"\n",
    "    Merge all .h5ad and .csv files from every sample in all timepoint subfolders\n",
    "    into a single experiment-level .h5ad and .csv file.\n",
    "\n",
    "    Parameters:\n",
    "        root_folder (str): Root directory containing timepoint folders.\n",
    "        output_name (str): Prefix for the output files (no extension).\n",
    "    \"\"\"\n",
    "    merged_adata_list = []\n",
    "    merged_summary_stats = []\n",
    "    merged_cellwise_spots = []\n",
    "\n",
    "    subfolders = sorted([\n",
    "        f for f in os.listdir(root_folder)\n",
    "        if os.path.isdir(os.path.join(root_folder, f)) and not f.startswith(\"._\")\n",
    "    ])\n",
    "\n",
    "    print(f\"üîç Scanning {len(subfolders)} timepoints...\")\n",
    "\n",
    "    for tp in subfolders:\n",
    "        tp_output = os.path.join(root_folder, tp, \"analysis_output\")\n",
    "        if not os.path.exists(tp_output):\n",
    "            continue\n",
    "\n",
    "        files = os.listdir(tp_output)\n",
    "        h5ad_files = [f for f in files if f.endswith(\"_nuclei_spots.h5ad\") and not f.startswith(\"._\")]\n",
    "\n",
    "        for h5ad_file in h5ad_files:\n",
    "            sample_base = h5ad_file.replace(\"_nuclei_spots.h5ad\", \"\")\n",
    "            h5ad_path = os.path.join(tp_output, h5ad_file)\n",
    "            stats_csv = os.path.join(tp_output, f\"{sample_base}_summary_statistics.csv\")\n",
    "            spot_csv = os.path.join(tp_output, f\"{sample_base}_cellwise_spot_counts.csv\")\n",
    "\n",
    "            try:\n",
    "                adata = anndata.read_h5ad(h5ad_path)\n",
    "                adata.obs[\"timepoint\"] = tp\n",
    "                merged_adata_list.append(adata)\n",
    "\n",
    "                if os.path.exists(stats_csv):\n",
    "                    df_stats = pd.read_csv(stats_csv)\n",
    "                    df_stats[\"timepoint\"] = tp\n",
    "                    df_stats[\"sample_name\"] = sample_base\n",
    "                    merged_summary_stats.append(df_stats)\n",
    "\n",
    "                if os.path.exists(spot_csv):\n",
    "                    df_spots = pd.read_csv(spot_csv)\n",
    "                    df_spots[\"timepoint\"] = tp\n",
    "                    df_spots[\"sample_name\"] = sample_base\n",
    "                    merged_cellwise_spots.append(df_spots)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {sample_base} in {tp}: {e}\")\n",
    "\n",
    "    # Merge AnnData\n",
    "    if not merged_adata_list:\n",
    "        print(\"‚ö†Ô∏è No .h5ad files successfully loaded.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîó Merging AnnData objects...\")\n",
    "    merged_adata = anndata.concat(merged_adata_list, axis=0, join=\"outer\", merge=\"unique\")\n",
    "\n",
    "    # Merge CSVs\n",
    "    summary_df = pd.concat(merged_summary_stats, ignore_index=True) if merged_summary_stats else pd.DataFrame()\n",
    "    spots_df = pd.concat(merged_cellwise_spots, ignore_index=True) if merged_cellwise_spots else pd.DataFrame()\n",
    "\n",
    "    # Save results\n",
    "    output_dir = os.path.join(root_folder, \"experiment_output_corrected\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    adata_path = os.path.join(output_dir, f\"{output_name}.h5ad\")\n",
    "    summary_csv = os.path.join(output_dir, f\"{output_name}_summary_statistics_SNR2.csv\")\n",
    "    spots_csv = os.path.join(output_dir, f\"{output_name}_cellwise_spot_counts_SNR2.csv\")\n",
    "\n",
    "    merged_adata.write(adata_path)\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    spots_df.to_csv(spots_csv, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Experiment-level files saved:\")\n",
    "    print(f\"üìÅ Merged AnnData: {adata_path}\")\n",
    "    print(f\"üìä Summary CSV:    {summary_csv}\")\n",
    "    print(f\"üìä Cellwise CSV:   {spots_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run it ----\n",
    "root_folder = 'path/to/single/replicate'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = root_folder  # root folder containing timepoint folders like \"0h\", \"1h\", etc.\n",
    "    merge_experiment(root, output_name=\"experiment_eglante\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigfish_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
